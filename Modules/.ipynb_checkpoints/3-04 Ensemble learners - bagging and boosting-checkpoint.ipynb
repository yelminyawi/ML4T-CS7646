{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensemble learners refer to combining algorithms\n",
    "\n",
    "2. Ensemble learners\n",
    "\n",
    "Using several algorithms or models, we input an x with the train data and then get a y. Each algorithm, say KNN\n",
    "Lin Reg, Decision Tree, and SVM produces its own answer with some degree of difference. What is typically done next\n",
    "is we average the y values to get a new set of predicted data for the model.\n",
    "\n",
    "This is more advantagous because each model has some sort of bias and this way we can combat the bias in some way.\n",
    "\n",
    "3. How to build an ensemble\n",
    "\n",
    "We could train parameterized polynomials of differing degree and combine that with training several KNN models using \n",
    "different subsets of data.\n",
    "\n",
    "4. Bootstrap aggregating bagging\n",
    "\n",
    "Instead of combining multiple algorithms, we could create an ensemble by creating different bags of data randomly selected \n",
    "from our train data (with replacement) and use m number of bags to train the data and query x values across each model to get \n",
    "a y value and then take their average\n",
    "\n",
    "5. Which will more likely overfit?  Single 1NN model using all data or ensemble of 1NN learners trained on 60% data.\n",
    "\n",
    "Answer: the Single 1NN model using all data will overfit. The ensemble with overfit individually, but once combined, \n",
    "the models becomes smoother.\n",
    "\n",
    "\n",
    "7. Boosting\n",
    "\n",
    "AdaBoost - in each bag we build our model then find some data gets modeled poorly. These datapoints get weighted differently\n",
    "than others so that in the next bag they are likely to get picked. Once that second bag is created, an ensemble model is made\n",
    "and tested against all the data to find the error. The same thing is done where data that is modeled poorly is weighted higher.\n",
    "\n",
    "This is Adaptive Boosting, or AdaBoost\n",
    "\n",
    "8. Overfitting - is Simple Boosting or AdaBoosting more likely to\n",
    "\n",
    "AdaBoost is more likely because it is striving to match the model at outliers\n",
    "\n",
    "9. Summary\n",
    "\n",
    "Ensemble learners, bagging and boosting are not new algorithms, they are metaalgorithms that let you wrap underlying algorithms\n",
    "into something better that will have less error and less overfitting\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
