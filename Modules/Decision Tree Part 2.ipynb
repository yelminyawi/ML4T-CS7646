{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Leaves are the values that you are trying to predict\n",
    "- Each decision node asks a binary question that leads to another node \n",
    "- decision trees can be visualized with as a traditional tree data structure\n",
    "- but we are going to represent/implement it in a tabular view, using a Numpy Array\n",
    "  DONT FORGET numpy arrays do not allow non-numerical values in them. So for say leaves\n",
    "  use a number to represent a leaf node\n",
    "- 00:24:00 around the time discussing Quinlans Tree that we are implementing \n",
    "- How to determine the best feature?\n",
    "    we need to determine the feature that separates the data best. We need to group the data into the most similar groups\n",
    "    Approches:\n",
    "    Information Gain: Entropy\n",
    "    Information Gain: Correlation - This is what we are using in this project\n",
    "    Information Gain: Gini Index\n",
    "    \n",
    "- So for the project\n",
    "  - (00:36:00) we choose which factor to split on by finding the correlation each column has with Y and whichever is highest\n",
    "      is the one we split on at the median. (He says this is done for us using comprehension so review the Quinlan Alg)\n",
    "  - we are going to repeat this approach until there are only two elements in the subtree, in this we can randomly choose which \n",
    "   to split on or make a deterministic choice \n",
    "\n",
    "- determining the median is the most expensive step of the Quinlan Alg\n",
    "\n",
    "- Random Tree Algorithm \n",
    "    - instead of finding the correlation every time to decide what value to split on, lets just choose a random one\n",
    "    - will this impair my tree - YES but only if you only have one tree, we are going to create an ensemble of trees\n",
    "    \n",
    "- Strengths and weaknesses of decision tree learners\n",
    "    - cost of learning is expensive compared to KNN learners. Linear regression is somewhere in between.\n",
    "    - decision forrest is the ensemble of trees, its cost of learning is multiplied by number of trees\n",
    "    - cost to query KNN < Decision Tree < Linear Regression\n",
    "    - For decision trees, you dont have to normalize data. You do with KNN otherwise highest data points will take over as \n",
    "    most important \n",
    "    - Reason we dont need to normalize is that we are already using correlation and then finding the median\n",
    "    \n",
    "USEFUL SLACK NOTES:\n",
    "We use an Numpy array to implement tree\n",
    "Number of split vals we need based on the number of leafs is 2N-1? Not sure how accurate or even useful this is.\n",
    "For leaf nodes using NaN is probably best\n",
    "\n",
    "\n",
    "in grade_learner comment out the test cases you dont need (which is most of them for DTLearner), and it will make sure your \n",
    "separate learners will work in a test environment. running PYTHONPATH=../:. python grade_learners.py  \n",
    "worked for me (edited) \n",
    "    \n",
    "looks like using np.shape[0] is different depending on the dimensions of the array/matrix. a 1-D array gives its vol length \n",
    "while a 2 D array will give it's row length\n",
    "\n",
    "that one tripped me up for hours too. its frustrating since doing shape[0] for a 2d array gives what youâ€™d \n",
    "expect, number of rows. I later used np.atleast_2d to avoid the issue\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
